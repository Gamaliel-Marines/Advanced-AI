# Gamaliel Marines Olvera - A01708746
# 09/10/2024

"""
- Implement a linear regression algorithm using gradient descent to make predictions on a dataset.

- This program uses a simple linear regression algorithm to predict the value of a dependent variable based on the value of an independent variable. 
The algorithm uses gradient descent to minimize the error between the predicted values and the actual values in the dataset.

-  This programm  will count with the following functions:
    - hyp: This function evaluates a generic linear function hyp(x) with current parameters.  hyp stands for hypothesis
    - devition: This function appends the deviation that are generated by the estimated values of hyp and the real value 'y'
    - grad: This function implements the Gradient Descent algorithm
    - scaling(samples): This function normalizes sample values so that gradient descent can converge
	- read_data: This function reads the data from a file and returns the samples and the target values
	- write_data: This function writes the data to a file (verify that the data is being written correctly)


"""

# import numpy library to use numpy arrays
import numpy

# import matplotlib library to generate graphs
import matplotlib.pyplot as plt

# import random library to generate random number -> for alpha (learing rate)
import random

# import csv library to read and write csv files
import csv


# Open the CSV file and read the column names
with open('Metro_Interstate_Traffic_Volume.csv', mode='r') as file:
    csv_reader = csv.reader(file)
    columns = next(csv_reader)
    print("Columns:", columns)


# Open the CSV file
def write_data():
    # Open the original CSV file
    with open('Metro_Interstate_Traffic_Volume.csv', mode='r') as csv_file:
        csv_reader = csv.reader(csv_file)
        columns = next(csv_reader)  # Skip the header row

        # Open the text file to write data
        with open("data.txt", "w") as file:
            for row in csv_reader:
                # Example: Write specific columns to the file (e.g., 'temp' and 'traffic_volume')
                temp = row[columns.index('temp')]
                traffic_volume = row[columns.index('traffic_volume')]
                file.write(f"{temp},{traffic_volume}\n")

write_data()


#global variable to store the deviations
__Devs__= [];

def h(params, sample):
	acum = 0
	for i in range(len(params)):
		acum = acum + params[i]*sample[i]
	return acum;


def show_errors(params, samples,y):
	global __Devs__
	error_acum =0

	for i in range(len(samples)):
		hypothesis = h(params,samples[i])
		print( "hypothesis  %f  y %f " % (hypothesis,  y[i]))   
		error=hypothesis-y[i]
		error_acum=+error**2
	mean_error_param=error_acum/len(samples)
	__Devs__.append(mean_error_param)

def GD(params, samples, y, alfa):
	temp = list(params)
	general_error=0
	for j in range(len(params)):
		acum =0; error_acum=0
		for i in range(len(samples)):
			error = h(params,samples[i]) - y[i]
			acum = acum + error*samples[i][j]
		temp[j] = params[j] - alfa*(1/len(samples))*acum
	return temp

def scaling(samples):
	acum =0
	samples = numpy.asarray(samples).T.tolist() 
	for i in range(1,len(samples)):	
		for j in range(len(samples[i])):
			acum=+ samples[i][j]
		avg = acum/(len(samples[i]))
		max_val = max(samples[i])
		for j in range(len(samples[i])):
			samples[i][j] = (samples[i][j] - avg)/max_val
	return numpy.asarray(samples).T.tolist()


#  univariate example
#params = [0,0]
#samples = [1,2,3,4,5]
#y = [2,4,6,8,10]

#  multivariate example trivial
params = [0,0,0]
samples = [[1,1],[2,2],[3,3],[4,4],[5,5]]
y = [2,4,6,8,10]


#  multivariate example
#params = [0,0,0]
#samples = [[1,1],[2,2],[3,3],[4,4],[5,5],[2,2],[3,3],[4,4]]
#y = [2,4,6,8,10,2,5.5,16]


def generate_learningRate():
	alfa = random.uniform(0.01, 0.3)
	return alfa


for i in range(len(samples)):
	if isinstance(samples[i], list):
		samples[i]=  [1]+samples[i]
	else:
		samples[i]=  [1,samples[i]]
print ("original samples:")
print (samples)
samples = scaling(samples)
print ("scaled samples:")
print (samples)


epochs = 0

while True:  #  run gradient descent until local minima is reached
	oldparams = list(params)
	print (params)
	alfa = generate_learningRate()

	write_data
	print(f"Current learning rate: {alfa}")
	params=GD(params, samples,y,alfa)	
	show_errors(params, samples, y)  #only used to show errors, it is not used in calculation
	print (params)
	epochs = epochs + 1
	if(oldparams == params or epochs == 2):   #  local minima is found when there is no further improvement
		print ("samples:")
		print(samples)
		print ("final params:")
		print (params)
		break

  #use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)
plt.plot(__Devs__)
plt.show()